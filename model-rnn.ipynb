{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female       0\n",
       "age          0\n",
       "height       0\n",
       "mass         0\n",
       "ta_set       0\n",
       "rh_set       0\n",
       "tre_int      0\n",
       "mtsk_int     0\n",
       "id_all       0\n",
       "unique_id    0\n",
       "study        0\n",
       "condition    0\n",
       "time         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import dataset\n",
    "df = pd.read_csv('./dataset/230322_OlderPredictTc_data_thermal.csv')\n",
    "\n",
    "# Only use previous values from same individual\n",
    "# df['previous_tre_int'] = df.groupby('id_all')['tre_int'].shift(1)\n",
    "# df['previous_mtsk_int'] = df.groupby('id_all')['mtsk_int'].shift(1)\n",
    "\n",
    "# Select only time > 0\n",
    "df = df[df.time > 0]\n",
    "\n",
    "# Unique ID to identify an individual\n",
    "df['unique_id'] = df['study'].astype(str) + '_' + df['condition'].astype(str) + '_' + df['id_all'].astype(str)\n",
    "\n",
    "# Select only features and output\n",
    "features = ['female', 'age', 'height', 'mass', 'ta_set', 'rh_set']\n",
    "output = ['tre_int', 'mtsk_int']\n",
    "df = df[features + output + ['id_all', 'unique_id', 'study', 'condition', 'time']]\n",
    "\n",
    "# Create train_df based on participants assigned to training set\n",
    "train_ids = [46, 34, 68, 30, 40, 98, 89, 65, 24, 58, 85, 67, 28, 39, 35, 77, 26,\n",
    "             80, 70, 37, 52, 56, 74, 78, 71, 60, 86, 43, 91, 82, 22, 59, 21, 87,\n",
    "             95, 66, 44, 25, 76, 94, 53, 32, 73, 23, 49]\n",
    "train_df = df[df['id_all'].isin(train_ids)]\n",
    "\n",
    "# Check data\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_extra_data(input_df):\n",
    "    # Create extra data for each participant\n",
    "    # This simulates sitting in air conditioning for 60 minutes before the trial\n",
    "    extra_data = pd.DataFrame()\n",
    "    for uid in input_df['unique_id'].unique():\n",
    "        participant_data = input_df[input_df['unique_id'] == uid].iloc[0]\n",
    "        new_data = pd.DataFrame({\n",
    "            'female': [participant_data['female']] * 60,\n",
    "            'age': [participant_data['age']] * 60,\n",
    "            'height': [participant_data['height']] * 60,\n",
    "            'mass': [participant_data['mass']] * 60,\n",
    "            'ta_set': [23] * 60,\n",
    "            'rh_set': [9] * 60,\n",
    "            'tre_int': [participant_data['tre_int']] * 60,\n",
    "            'mtsk_int': [participant_data['mtsk_int']] * 60,\n",
    "            'id_all': [participant_data['id_all']] * 60,\n",
    "            'unique_id': [participant_data['unique_id']] * 60,\n",
    "            'study': [participant_data['study']] * 60,\n",
    "            'condition': [participant_data['condition']] * 60,\n",
    "            'time': list(range(-60, 0))\n",
    "        })\n",
    "        extra_data = extra_data.append(new_data, ignore_index=True)\n",
    "    return extra_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/byh6pjkx74d7p_crfrzgdpbd2xgt0t/T/ipykernel_18740/2786065655.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  extra_data = extra_data.append(new_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 600\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s5068337/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - loss: 0.5547 - val_loss: 0.2871\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - loss: 0.2802 - val_loss: 0.0958\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step - loss: 0.1073 - val_loss: 0.0272\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - loss: 0.0466 - val_loss: 0.0787\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - loss: 0.0705 - val_loss: 0.0431\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - loss: 0.0397 - val_loss: 0.0210\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239ms/step - loss: 0.0205 - val_loss: 0.0272\n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332ms/step - loss: 0.0242 - val_loss: 0.0358\n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241ms/step - loss: 0.0275 - val_loss: 0.0354\n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 242ms/step - loss: 0.0256 - val_loss: 0.0284\n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 0.0195 - val_loss: 0.0217\n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - loss: 0.0159 - val_loss: 0.0192\n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 0.0166 - val_loss: 0.0187\n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.0178 - val_loss: 0.0183\n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 0.0175 - val_loss: 0.0183\n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step - loss: 0.0156 - val_loss: 0.0195\n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.0153 - val_loss: 0.0208\n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step - loss: 0.0154 - val_loss: 0.0210\n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303ms/step - loss: 0.0152 - val_loss: 0.0202\n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step - loss: 0.0149 - val_loss: 0.0189\n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - loss: 0.0144 - val_loss: 0.0178\n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step - loss: 0.0147 - val_loss: 0.0174\n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244ms/step - loss: 0.0147 - val_loss: 0.0174\n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step - loss: 0.0144 - val_loss: 0.0175\n",
      "Epoch 25/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step - loss: 0.0146 - val_loss: 0.0176\n",
      "Epoch 26/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - loss: 0.0144 - val_loss: 0.0175\n",
      "Epoch 27/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - loss: 0.0139 - val_loss: 0.0172\n",
      "Epoch 28/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - loss: 0.0142 - val_loss: 0.0170\n",
      "Epoch 29/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step - loss: 0.0138 - val_loss: 0.0172\n",
      "Epoch 30/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step - loss: 0.0138 - val_loss: 0.0173\n",
      "Epoch 31/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - loss: 0.0140 - val_loss: 0.0172\n",
      "Epoch 32/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step - loss: 0.0135 - val_loss: 0.0172\n",
      "Epoch 33/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 242ms/step - loss: 0.0137 - val_loss: 0.0171\n",
      "Epoch 34/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239ms/step - loss: 0.0133 - val_loss: 0.0166\n",
      "Epoch 35/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - loss: 0.0135 - val_loss: 0.0163\n",
      "Epoch 36/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 237ms/step - loss: 0.0135 - val_loss: 0.0163\n",
      "Epoch 37/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 237ms/step - loss: 0.0135 - val_loss: 0.0163\n",
      "Epoch 38/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225ms/step - loss: 0.0133 - val_loss: 0.0162\n",
      "Epoch 39/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - loss: 0.0134 - val_loss: 0.0163\n",
      "Epoch 40/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 234ms/step - loss: 0.0131 - val_loss: 0.0164\n",
      "Epoch 41/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step - loss: 0.0131 - val_loss: 0.0163\n",
      "Epoch 42/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step - loss: 0.0130 - val_loss: 0.0160\n",
      "Epoch 43/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.0128 - val_loss: 0.0159\n",
      "Epoch 44/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239ms/step - loss: 0.0129 - val_loss: 0.0161\n",
      "Epoch 45/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - loss: 0.0130 - val_loss: 0.0163\n",
      "Epoch 46/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256ms/step - loss: 0.0124 - val_loss: 0.0161\n",
      "Epoch 47/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - loss: 0.0125 - val_loss: 0.0161\n",
      "Epoch 48/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 49/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 0.0126 - val_loss: 0.0160\n",
      "Epoch 50/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.0127 - val_loss: 0.0157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x12d3472e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "# Reset index\n",
    "train_df.reset_index(inplace=True)\n",
    "\n",
    "# 60 mins of data in aircon\n",
    "extra_data = generate_extra_data(train_df)\n",
    "# Concatenate extra data with train_df\n",
    "train_df = pd.concat([extra_data, train_df], ignore_index=True)\n",
    "\n",
    "# Scalars\n",
    "features_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "output_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# Fit scalers\n",
    "X_scaled = features_scaler.fit_transform(train_df[features])\n",
    "y_scaled = output_scaler.fit_transform(train_df[output])\n",
    "\n",
    "# For each feature row, we need to map it so that\n",
    "# Create sequences based on unique_id\n",
    "unique_ids = train_df['unique_id'].unique()\n",
    "X_seq, y_seq = [], []\n",
    "for uid in unique_ids:\n",
    "    seq_data = train_df[train_df['unique_id'] == uid]\n",
    "    X_seq.append(X_scaled[seq_data.index])\n",
    "    y_seq.append(y_scaled[seq_data.index])\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_len = max(len(seq) for seq in X_seq)\n",
    "print(\"Max sequence length:\", max_len)\n",
    "X_padded = np.array([np.pad(seq, ((0, max_len - len(seq)), (0, 0)), mode='constant') for seq in X_seq])\n",
    "y_padded = np.array([np.pad(seq, ((0, max_len - len(seq)), (0, 0)), mode='constant') for seq in y_seq])\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(None, X_padded.shape[-1]), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(y_padded.shape[-1])))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_padded, y_padded, epochs=50, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATE\n",
    "from helpers import get_sample\n",
    "\n",
    "def run_and_save_trial(study, condition):\n",
    "    # Get sample\n",
    "    sample = get_sample(study, condition)\n",
    "\n",
    "    # Unique ID to identify an individual\n",
    "    sample['unique_id'] = sample['study'].astype(str) + '_' + sample['condition'].astype(str) + '_' + sample['id_all'].astype(str)\n",
    "\n",
    "    extra_data = generate_extra_data(sample)\n",
    "    # Concatenate extra data with train_df\n",
    "    sample_extra_data = pd.concat([extra_data, sample], ignore_index=True)\n",
    "\n",
    "    # Fit scalers\n",
    "    all_X_scaled = features_scaler.fit_transform(sample_extra_data[features])\n",
    "\n",
    "    # Create sequences based on unique_id\n",
    "    all_unique_ids = sample_extra_data['unique_id'].unique()\n",
    "    all_X_seq = []\n",
    "    seq_lengths = []  # Store the original sequence lengths\n",
    "\n",
    "    for uid in all_unique_ids:\n",
    "        seq_data = sample_extra_data['unique_id'] == uid\n",
    "        data_for_uid = all_X_scaled[seq_data]\n",
    "        all_X_seq.append(data_for_uid)\n",
    "        seq_lengths.append(len(data_for_uid))  # Store the original sequence length\n",
    "\n",
    "    # Pad sequences to have the same length\n",
    "    all_X_padded = np.array([np.pad(seq, ((0, max_len - len(seq)), (0, 0)), mode='constant') for seq in all_X_seq])\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(all_X_padded)\n",
    "\n",
    "    # Remove predictions corresponding to padded inputs and extra data\n",
    "    unpadded_predictions = []\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        unpadded_predictions.append(predictions[i, 60:length])  # Slice to remove 60 mins of extra data\n",
    "\n",
    "    # Flatten the unpadded predictions\n",
    "    unpadded_predictions = np.concatenate(unpadded_predictions, axis=0)\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    unpadded_predictions = output_scaler.inverse_transform(unpadded_predictions)\n",
    "\n",
    "    all_core_temps = unpadded_predictions[:, 0]\n",
    "    all_skin_temps = unpadded_predictions[:, 1]\n",
    "\n",
    "    print(all_core_temps.shape[0])\n",
    "    print(all_skin_temps.shape[0])\n",
    "    print(sample.shape[0])\n",
    "\n",
    "    # Save to csv\n",
    "    df = pd.DataFrame(all_core_temps, columns=[\"tre_predicted\"])\n",
    "    df[\"mtsk_predicted\"] = all_skin_temps\n",
    "    df.to_csv('results/rnn-{}-{}.csv'.format(study, condition), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/byh6pjkx74d7p_crfrzgdpbd2xgt0t/T/ipykernel_18740/2786065655.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  extra_data = extra_data.append(new_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "31860\n",
      "31860\n",
      "31860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/byh6pjkx74d7p_crfrzgdpbd2xgt0t/T/ipykernel_18740/2786065655.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  extra_data = extra_data.append(new_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "7680\n",
      "7680\n",
      "7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/byh6pjkx74d7p_crfrzgdpbd2xgt0t/T/ipykernel_18740/2786065655.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  extra_data = extra_data.append(new_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "7680\n",
      "7680\n",
      "7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/byh6pjkx74d7p_crfrzgdpbd2xgt0t/T/ipykernel_18740/2786065655.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  extra_data = extra_data.append(new_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "17760\n",
      "17760\n",
      "17760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/byh6pjkx74d7p_crfrzgdpbd2xgt0t/T/ipykernel_18740/2786065655.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  extra_data = extra_data.append(new_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "16800\n",
      "16800\n",
      "16800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/byh6pjkx74d7p_crfrzgdpbd2xgt0t/T/ipykernel_18740/2786065655.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  extra_data = extra_data.append(new_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "10260\n",
      "10260\n",
      "10260\n"
     ]
    }
   ],
   "source": [
    "run_and_save_trial('heatwave 1 (prolonged)', 'hot')\n",
    "run_and_save_trial('heatwave 2 (indoor)', 'cool')\n",
    "run_and_save_trial('heatwave 2 (indoor)', 'temp')\n",
    "run_and_save_trial('heatwave 2 (indoor)', 'warm')\n",
    "run_and_save_trial('heatwave 2 (indoor)', 'hot')\n",
    "run_and_save_trial('heatwave 3 (cooling)', 'hot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
